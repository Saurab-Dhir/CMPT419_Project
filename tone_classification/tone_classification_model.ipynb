{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tone Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "from data_loader import extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "\n",
    "wav_files_list = [f for f in os.listdir(data_dir) if f.endswith('.wav')]\n",
    "# print(wav_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/1001_DFA_ANG_XX.wav</td>\n",
       "      <td>ANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/1001_DFA_DIS_XX.wav</td>\n",
       "      <td>DIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/1001_DFA_FEA_XX.wav</td>\n",
       "      <td>FEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/1001_DFA_HAP_XX.wav</td>\n",
       "      <td>HAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/1001_DFA_NEU_XX.wav</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file label\n",
       "0  data/1001_DFA_ANG_XX.wav   ANG\n",
       "1  data/1001_DFA_DIS_XX.wav   DIS\n",
       "2  data/1001_DFA_FEA_XX.wav   FEA\n",
       "3  data/1001_DFA_HAP_XX.wav   HAP\n",
       "4  data/1001_DFA_NEU_XX.wav   NEU"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the label out of file names\n",
    "data = []\n",
    "for file in wav_files_list:\n",
    "    parts = file.split('_')\n",
    "    if len(parts) > 2:\n",
    "        label = parts[2]\n",
    "        filepath = os.path.join(data_dir, file)\n",
    "        data.append({'file': filepath, 'label': label})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tone Classification model\n",
    "\n",
    "For lightweight tone classification we will use SVM and XGBoost. \n",
    "We use LSTMs for sequential tone analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    features = extract_features(row['file'])\n",
    "    if features is not None:\n",
    "        X.append(features)\n",
    "        y.append(row['label'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode lables (converts emotions to numbers cause SVM only takes numerical values)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape for LSTM (samples, time_steps=1, features)\n",
    "X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_lstm, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMToneClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMToneClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        return self.fc(hn[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch 01/15 | Train Loss: 1.5737 | Val Loss: 1.4608 | Val Acc: 0.4163\n",
      "Epoch 02/15 | Train Loss: 1.4061 | Val Loss: 1.3891 | Val Acc: 0.4512\n",
      "Epoch 03/15 | Train Loss: 1.3552 | Val Loss: 1.3615 | Val Acc: 0.4682\n",
      "Epoch 04/15 | Train Loss: 1.3330 | Val Loss: 1.3464 | Val Acc: 0.4790\n",
      "Epoch 05/15 | Train Loss: 1.3174 | Val Loss: 1.3366 | Val Acc: 0.4754\n",
      "Epoch 06/15 | Train Loss: 1.3036 | Val Loss: 1.3310 | Val Acc: 0.4727\n",
      "Epoch 07/15 | Train Loss: 1.2939 | Val Loss: 1.3248 | Val Acc: 0.4709\n",
      "Epoch 08/15 | Train Loss: 1.2832 | Val Loss: 1.3186 | Val Acc: 0.4861\n",
      "Epoch 09/15 | Train Loss: 1.2742 | Val Loss: 1.3122 | Val Acc: 0.4790\n",
      "Epoch 10/15 | Train Loss: 1.2670 | Val Loss: 1.3065 | Val Acc: 0.4745\n",
      "Epoch 11/15 | Train Loss: 1.2570 | Val Loss: 1.3037 | Val Acc: 0.4781\n",
      "Epoch 12/15 | Train Loss: 1.2493 | Val Loss: 1.3004 | Val Acc: 0.4781\n",
      "Epoch 13/15 | Train Loss: 1.2419 | Val Loss: 1.2943 | Val Acc: 0.4754\n",
      "Epoch 14/15 | Train Loss: 1.2360 | Val Loss: 1.2903 | Val Acc: 0.4790\n",
      "Epoch 15/15 | Train Loss: 1.2285 | Val Loss: 1.2871 | Val Acc: 0.4799\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ANG       0.62      0.64      0.63       191\n",
      "         DIS       0.41      0.39      0.40       190\n",
      "         FEA       0.49      0.32      0.38       190\n",
      "         HAP       0.44      0.47      0.45       191\n",
      "         NEU       0.39      0.45      0.42       164\n",
      "         SAD       0.52      0.60      0.56       191\n",
      "\n",
      "    accuracy                           0.48      1117\n",
      "   macro avg       0.48      0.48      0.47      1117\n",
      "weighted avg       0.48      0.48      0.48      1117\n",
      "\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch 01/15 | Train Loss: 1.5641 | Val Loss: 1.4466 | Val Acc: 0.4059\n",
      "Epoch 02/15 | Train Loss: 1.4031 | Val Loss: 1.3752 | Val Acc: 0.4355\n",
      "Epoch 03/15 | Train Loss: 1.3555 | Val Loss: 1.3501 | Val Acc: 0.4498\n",
      "Epoch 04/15 | Train Loss: 1.3337 | Val Loss: 1.3389 | Val Acc: 0.4471\n",
      "Epoch 05/15 | Train Loss: 1.3187 | Val Loss: 1.3270 | Val Acc: 0.4588\n",
      "Epoch 06/15 | Train Loss: 1.3019 | Val Loss: 1.3204 | Val Acc: 0.4534\n",
      "Epoch 07/15 | Train Loss: 1.2941 | Val Loss: 1.3118 | Val Acc: 0.4615\n",
      "Epoch 08/15 | Train Loss: 1.2796 | Val Loss: 1.3105 | Val Acc: 0.4597\n",
      "Epoch 09/15 | Train Loss: 1.2692 | Val Loss: 1.3117 | Val Acc: 0.4677\n",
      "Epoch 10/15 | Train Loss: 1.2631 | Val Loss: 1.3055 | Val Acc: 0.4722\n",
      "Epoch 11/15 | Train Loss: 1.2530 | Val Loss: 1.2979 | Val Acc: 0.4713\n",
      "Epoch 12/15 | Train Loss: 1.2466 | Val Loss: 1.2984 | Val Acc: 0.4749\n",
      "Epoch 13/15 | Train Loss: 1.2454 | Val Loss: 1.2942 | Val Acc: 0.4722\n",
      "Epoch 14/15 | Train Loss: 1.2349 | Val Loss: 1.2931 | Val Acc: 0.4785\n",
      "Epoch 15/15 | Train Loss: 1.2245 | Val Loss: 1.2905 | Val Acc: 0.4785\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ANG       0.64      0.62      0.63       190\n",
      "         DIS       0.38      0.36      0.37       191\n",
      "         FEA       0.39      0.27      0.32       190\n",
      "         HAP       0.51      0.48      0.49       191\n",
      "         NEU       0.42      0.50      0.46       163\n",
      "         SAD       0.50      0.65      0.56       191\n",
      "\n",
      "    accuracy                           0.48      1116\n",
      "   macro avg       0.47      0.48      0.47      1116\n",
      "weighted avg       0.47      0.48      0.47      1116\n",
      "\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch 01/15 | Train Loss: 1.5713 | Val Loss: 1.4587 | Val Acc: 0.4059\n",
      "Epoch 02/15 | Train Loss: 1.4067 | Val Loss: 1.3814 | Val Acc: 0.4453\n",
      "Epoch 03/15 | Train Loss: 1.3552 | Val Loss: 1.3584 | Val Acc: 0.4579\n",
      "Epoch 04/15 | Train Loss: 1.3352 | Val Loss: 1.3450 | Val Acc: 0.4525\n",
      "Epoch 05/15 | Train Loss: 1.3147 | Val Loss: 1.3384 | Val Acc: 0.4570\n",
      "Epoch 06/15 | Train Loss: 1.3043 | Val Loss: 1.3298 | Val Acc: 0.4615\n",
      "Epoch 07/15 | Train Loss: 1.2909 | Val Loss: 1.3251 | Val Acc: 0.4606\n",
      "Epoch 08/15 | Train Loss: 1.2791 | Val Loss: 1.3174 | Val Acc: 0.4588\n",
      "Epoch 09/15 | Train Loss: 1.2726 | Val Loss: 1.3139 | Val Acc: 0.4642\n",
      "Epoch 10/15 | Train Loss: 1.2630 | Val Loss: 1.3091 | Val Acc: 0.4677\n",
      "Epoch 11/15 | Train Loss: 1.2568 | Val Loss: 1.3062 | Val Acc: 0.4713\n",
      "Epoch 12/15 | Train Loss: 1.2443 | Val Loss: 1.3012 | Val Acc: 0.4642\n",
      "Epoch 13/15 | Train Loss: 1.2394 | Val Loss: 1.3009 | Val Acc: 0.4713\n",
      "Epoch 14/15 | Train Loss: 1.2329 | Val Loss: 1.2968 | Val Acc: 0.4677\n",
      "Epoch 15/15 | Train Loss: 1.2224 | Val Loss: 1.2970 | Val Acc: 0.4713\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ANG       0.59      0.73      0.65       190\n",
      "         DIS       0.39      0.38      0.39       191\n",
      "         FEA       0.39      0.26      0.31       191\n",
      "         HAP       0.46      0.38      0.42       190\n",
      "         NEU       0.42      0.43      0.43       163\n",
      "         SAD       0.50      0.64      0.56       191\n",
      "\n",
      "    accuracy                           0.47      1116\n",
      "   macro avg       0.46      0.47      0.46      1116\n",
      "weighted avg       0.46      0.47      0.46      1116\n",
      "\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch 01/15 | Train Loss: 1.5779 | Val Loss: 1.4500 | Val Acc: 0.4149\n",
      "Epoch 02/15 | Train Loss: 1.4132 | Val Loss: 1.3747 | Val Acc: 0.4444\n",
      "Epoch 03/15 | Train Loss: 1.3659 | Val Loss: 1.3546 | Val Acc: 0.4498\n",
      "Epoch 04/15 | Train Loss: 1.3344 | Val Loss: 1.3409 | Val Acc: 0.4588\n",
      "Epoch 05/15 | Train Loss: 1.3180 | Val Loss: 1.3334 | Val Acc: 0.4642\n",
      "Epoch 06/15 | Train Loss: 1.3063 | Val Loss: 1.3221 | Val Acc: 0.4633\n",
      "Epoch 07/15 | Train Loss: 1.2921 | Val Loss: 1.3127 | Val Acc: 0.4758\n",
      "Epoch 08/15 | Train Loss: 1.2869 | Val Loss: 1.3058 | Val Acc: 0.4704\n",
      "Epoch 09/15 | Train Loss: 1.2683 | Val Loss: 1.3159 | Val Acc: 0.4722\n",
      "Epoch 10/15 | Train Loss: 1.2627 | Val Loss: 1.2964 | Val Acc: 0.4812\n",
      "Epoch 11/15 | Train Loss: 1.2525 | Val Loss: 1.2949 | Val Acc: 0.4892\n",
      "Epoch 12/15 | Train Loss: 1.2448 | Val Loss: 1.2907 | Val Acc: 0.4857\n",
      "Epoch 13/15 | Train Loss: 1.2403 | Val Loss: 1.2890 | Val Acc: 0.4884\n",
      "Epoch 14/15 | Train Loss: 1.2312 | Val Loss: 1.2852 | Val Acc: 0.4875\n",
      "Epoch 15/15 | Train Loss: 1.2290 | Val Loss: 1.2867 | Val Acc: 0.4821\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ANG       0.65      0.72      0.69       191\n",
      "         DIS       0.42      0.43      0.43       191\n",
      "         FEA       0.45      0.30      0.36       191\n",
      "         HAP       0.46      0.39      0.42       190\n",
      "         NEU       0.39      0.45      0.42       163\n",
      "         SAD       0.49      0.59      0.53       190\n",
      "\n",
      "    accuracy                           0.48      1116\n",
      "   macro avg       0.48      0.48      0.47      1116\n",
      "weighted avg       0.48      0.48      0.48      1116\n",
      "\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch 01/15 | Train Loss: 1.5692 | Val Loss: 1.4649 | Val Acc: 0.4220\n",
      "Epoch 02/15 | Train Loss: 1.4046 | Val Loss: 1.3943 | Val Acc: 0.4480\n",
      "Epoch 03/15 | Train Loss: 1.3526 | Val Loss: 1.3692 | Val Acc: 0.4453\n",
      "Epoch 04/15 | Train Loss: 1.3326 | Val Loss: 1.3515 | Val Acc: 0.4704\n",
      "Epoch 05/15 | Train Loss: 1.3128 | Val Loss: 1.3504 | Val Acc: 0.4668\n",
      "Epoch 06/15 | Train Loss: 1.3006 | Val Loss: 1.3401 | Val Acc: 0.4722\n",
      "Epoch 07/15 | Train Loss: 1.2917 | Val Loss: 1.3351 | Val Acc: 0.4606\n",
      "Epoch 08/15 | Train Loss: 1.2826 | Val Loss: 1.3274 | Val Acc: 0.4767\n",
      "Epoch 09/15 | Train Loss: 1.2754 | Val Loss: 1.3224 | Val Acc: 0.4758\n",
      "Epoch 10/15 | Train Loss: 1.2624 | Val Loss: 1.3210 | Val Acc: 0.4713\n",
      "Epoch 11/15 | Train Loss: 1.2591 | Val Loss: 1.3238 | Val Acc: 0.4704\n",
      "Epoch 12/15 | Train Loss: 1.2511 | Val Loss: 1.3162 | Val Acc: 0.4686\n",
      "Epoch 13/15 | Train Loss: 1.2386 | Val Loss: 1.3143 | Val Acc: 0.4722\n",
      "Epoch 14/15 | Train Loss: 1.2324 | Val Loss: 1.3123 | Val Acc: 0.4731\n",
      "Epoch 15/15 | Train Loss: 1.2277 | Val Loss: 1.3077 | Val Acc: 0.4758\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ANG       0.64      0.62      0.63       191\n",
      "         DIS       0.42      0.42      0.42       190\n",
      "         FEA       0.44      0.25      0.31       191\n",
      "         HAP       0.49      0.47      0.48       191\n",
      "         NEU       0.41      0.48      0.44       163\n",
      "         SAD       0.46      0.62      0.53       190\n",
      "\n",
      "    accuracy                           0.48      1116\n",
      "   macro avg       0.48      0.48      0.47      1116\n",
      "weighted avg       0.48      0.48      0.47      1116\n",
      "\n",
      "\n",
      "=== Average Accuracy over 5 folds: 0.4775 ± 0.0037\n"
     ]
    }
   ],
   "source": [
    "# === K-Fold Cross Validation on training set ===\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "input_dim = X_train.shape[2]\n",
    "hidden_dim = 64\n",
    "output_dim = len(np.unique(y_encoded))\n",
    "batch_size = 16\n",
    "epochs = 15\n",
    "\n",
    "fold_accuracies = []\n",
    "best_model = None\n",
    "best_acc = 0\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
    "\n",
    "    X_fold_train = X_train[train_idx]\n",
    "    y_fold_train = y_train[train_idx]\n",
    "    X_fold_val = X_train[val_idx]\n",
    "    y_fold_val = y_train[val_idx]\n",
    "\n",
    "    # Loaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_fold_train, dtype=torch.float32),\n",
    "                                  torch.tensor(y_fold_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_fold_val, dtype=torch.float32),\n",
    "                                torch.tensor(y_fold_val, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = LSTMToneClassifier(input_dim, hidden_dim, output_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        val_loss_total = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                outputs = model(xb)\n",
    "                loss = criterion(outputs, yb)\n",
    "                val_loss_total += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss_total / len(val_loader)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {acc:.4f}\")\n",
    "\n",
    "    # Fold summary\n",
    "    fold_acc = accuracy_score(all_labels, all_preds)\n",
    "    fold_accuracies.append(fold_acc)\n",
    "\n",
    "    if fold_acc > best_acc:\n",
    "        best_acc = fold_acc\n",
    "        best_model = model\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, target_names=le.classes_))\n",
    "\n",
    "print(f\"\\n=== Average Accuracy over {n_splits} folds: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "419env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
