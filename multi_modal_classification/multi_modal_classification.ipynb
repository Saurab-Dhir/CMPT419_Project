{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-modal Emotion Classification Model\n",
    "This model takes a late fusion approach to classify the overall emotion. The model takes in probability distribbutions from each classifiers (tone, facial expression, semantics) and learns the patterns from them to predict the overall emotion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/bin/python\n",
      "Requirement already satisfied: torch in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: pandas in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.6.1)\n",
      "Requirement already satisfied: numpy in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Requirement already satisfied: networkx in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (2025.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: filelock in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.4.2)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 5)) (24.2)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting pillow>=8\n",
      "  Using cached pillow-11.1.0-cp39-cp39-macosx_11_0_arm64.whl (3.1 MB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.56.0-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 5)) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/lib/python3.9/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.56.0 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pillow-11.1.0 pyparsing-3.2.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/rkawagoe/Documents/SFU/Courses/CMPT419/Project/CMPT419_Project/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "# !{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import ast\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 1000\n",
      "Total rows in dataset: 200\n"
     ]
    }
   ],
   "source": [
    "from data_loader import MultiModalEmotionDataset\n",
    "\n",
    "# Load data from csv file\n",
    "train_data_path = \"data/fusion_training_data.csv\"\n",
    "val_data_path = \"data/fusion_validation_data.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "val_data = pd.read_csv(val_data_path)\n",
    "\n",
    "modality_probs = ['tone_prediction', 'face_prediction', 'semantic_prediction']\n",
    "for modality in modality_probs:\n",
    "    train_data[modality] = train_data[modality].apply(ast.literal_eval)\n",
    "    val_data[modality] = val_data[modality].apply(ast.literal_eval)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = MultiModalEmotionDataset(dataframe=train_data, prob_columns=modality_probs, label_column='emotion')\n",
    "val_dataset = MultiModalEmotionDataset(dataframe=val_data, prob_columns=modality_probs, label_column='emotion')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateFusionEmotionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=18, output_dim=6):\n",
    "        super(LateFusionEmotionClassifier, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, criterion, train_loader, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (x_batch, y_batch, original_index) in enumerate(train_loader):\n",
    "\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = torch.argmax(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # statistics for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "def validate(model, device, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, original_index in val_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            _, batch_preds = torch.max(outputs, 1)\n",
    "            correct += (batch_preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            labels.extend(y_batch.cpu().numpy())\n",
    "            predictions.extend(batch_preds.cpu().numpy())\n",
    "\n",
    "    return labels, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation:\n",
      ">> Fold [1/5]\n",
      "Training:\n",
      "Epoch [1/100] Loss: 1.7996, Accuracy: 16.5000\n",
      "Epoch [2/100] Loss: 1.7961, Accuracy: 15.2500\n",
      "Epoch [3/100] Loss: 1.7927, Accuracy: 16.1250\n",
      "Epoch [4/100] Loss: 1.7908, Accuracy: 17.7500\n",
      "Epoch [5/100] Loss: 1.7903, Accuracy: 17.7500\n",
      "Epoch [6/100] Loss: 1.7891, Accuracy: 18.3750\n",
      "Epoch [7/100] Loss: 1.7879, Accuracy: 18.6250\n",
      "Epoch [8/100] Loss: 1.7865, Accuracy: 21.3750\n",
      "Epoch [9/100] Loss: 1.7850, Accuracy: 23.5000\n",
      "Epoch [10/100] Loss: 1.7854, Accuracy: 23.6250\n",
      "Epoch [11/100] Loss: 1.7822, Accuracy: 22.3750\n",
      "Epoch [12/100] Loss: 1.7800, Accuracy: 23.7500\n",
      "Epoch [13/100] Loss: 1.7777, Accuracy: 23.2500\n",
      "Epoch [14/100] Loss: 1.7757, Accuracy: 21.5000\n",
      "Epoch [15/100] Loss: 1.7730, Accuracy: 23.5000\n",
      "Epoch [16/100] Loss: 1.7720, Accuracy: 22.8750\n",
      "Epoch [17/100] Loss: 1.7664, Accuracy: 24.5000\n",
      "Epoch [18/100] Loss: 1.7648, Accuracy: 22.5000\n",
      "Epoch [19/100] Loss: 1.7636, Accuracy: 22.5000\n",
      "Epoch [20/100] Loss: 1.7603, Accuracy: 23.8750\n",
      "Epoch [21/100] Loss: 1.7623, Accuracy: 23.8750\n",
      "Epoch [22/100] Loss: 1.7525, Accuracy: 25.0000\n",
      "Epoch [23/100] Loss: 1.7503, Accuracy: 25.7500\n",
      "Epoch [24/100] Loss: 1.7480, Accuracy: 24.7500\n",
      "Epoch [25/100] Loss: 1.7465, Accuracy: 26.1250\n",
      "Epoch [26/100] Loss: 1.7475, Accuracy: 24.1250\n",
      "Epoch [27/100] Loss: 1.7378, Accuracy: 26.0000\n",
      "Epoch [28/100] Loss: 1.7360, Accuracy: 26.0000\n",
      "Epoch [29/100] Loss: 1.7373, Accuracy: 26.8750\n",
      "Epoch [30/100] Loss: 1.7292, Accuracy: 25.6250\n",
      "Epoch [31/100] Loss: 1.7265, Accuracy: 27.5000\n",
      "Epoch [32/100] Loss: 1.7228, Accuracy: 26.8750\n",
      "Epoch [33/100] Loss: 1.7219, Accuracy: 27.0000\n",
      "Epoch [34/100] Loss: 1.7174, Accuracy: 28.6250\n",
      "Epoch [35/100] Loss: 1.7145, Accuracy: 26.7500\n",
      "Epoch [36/100] Loss: 1.7064, Accuracy: 28.5000\n",
      "Epoch [37/100] Loss: 1.7028, Accuracy: 28.2500\n",
      "Epoch [38/100] Loss: 1.7112, Accuracy: 28.0000\n",
      "Epoch [39/100] Loss: 1.6968, Accuracy: 28.6250\n",
      "Epoch [40/100] Loss: 1.6968, Accuracy: 28.7500\n",
      "Epoch [41/100] Loss: 1.6887, Accuracy: 28.5000\n",
      "Epoch [42/100] Loss: 1.6838, Accuracy: 30.5000\n",
      "Epoch [43/100] Loss: 1.6841, Accuracy: 29.7500\n",
      "Epoch [44/100] Loss: 1.6780, Accuracy: 29.6250\n",
      "Epoch [45/100] Loss: 1.6628, Accuracy: 30.2500\n",
      "Epoch [46/100] Loss: 1.6677, Accuracy: 29.7500\n",
      "Epoch [47/100] Loss: 1.6639, Accuracy: 30.1250\n",
      "Epoch [48/100] Loss: 1.6577, Accuracy: 29.6250\n",
      "Epoch [49/100] Loss: 1.6550, Accuracy: 30.2500\n",
      "Epoch [50/100] Loss: 1.6445, Accuracy: 31.6250\n",
      "Epoch [51/100] Loss: 1.6420, Accuracy: 33.2500\n",
      "Epoch [52/100] Loss: 1.6286, Accuracy: 33.5000\n",
      "Epoch [53/100] Loss: 1.6209, Accuracy: 33.7500\n",
      "Epoch [54/100] Loss: 1.6194, Accuracy: 33.2500\n",
      "Epoch [55/100] Loss: 1.6129, Accuracy: 35.0000\n",
      "Epoch [56/100] Loss: 1.6047, Accuracy: 34.5000\n",
      "Epoch [57/100] Loss: 1.5992, Accuracy: 35.5000\n",
      "Epoch [58/100] Loss: 1.5910, Accuracy: 35.8750\n",
      "Epoch [59/100] Loss: 1.5778, Accuracy: 35.1250\n",
      "Epoch [60/100] Loss: 1.5796, Accuracy: 37.2500\n",
      "Epoch [61/100] Loss: 1.5781, Accuracy: 37.1250\n",
      "Epoch [62/100] Loss: 1.5648, Accuracy: 37.5000\n",
      "Epoch [63/100] Loss: 1.5564, Accuracy: 37.5000\n",
      "Epoch [64/100] Loss: 1.5544, Accuracy: 37.7500\n",
      "Epoch [65/100] Loss: 1.5399, Accuracy: 39.6250\n",
      "Epoch [66/100] Loss: 1.5478, Accuracy: 38.2500\n",
      "Epoch [67/100] Loss: 1.5392, Accuracy: 37.6250\n",
      "Epoch [68/100] Loss: 1.5186, Accuracy: 40.3750\n",
      "Epoch [69/100] Loss: 1.5136, Accuracy: 39.5000\n",
      "Epoch [70/100] Loss: 1.5163, Accuracy: 40.3750\n",
      "Epoch [71/100] Loss: 1.5013, Accuracy: 40.8750\n",
      "Epoch [72/100] Loss: 1.4994, Accuracy: 41.8750\n",
      "Epoch [73/100] Loss: 1.5130, Accuracy: 38.7500\n",
      "Epoch [74/100] Loss: 1.4739, Accuracy: 43.0000\n",
      "Epoch [75/100] Loss: 1.4721, Accuracy: 42.6250\n",
      "Epoch [76/100] Loss: 1.4540, Accuracy: 42.8750\n",
      "Epoch [77/100] Loss: 1.4583, Accuracy: 42.8750\n",
      "Epoch [78/100] Loss: 1.4491, Accuracy: 44.1250\n",
      "Epoch [79/100] Loss: 1.4459, Accuracy: 43.7500\n",
      "Epoch [80/100] Loss: 1.4352, Accuracy: 44.2500\n",
      "Epoch [81/100] Loss: 1.4388, Accuracy: 44.0000\n",
      "Epoch [82/100] Loss: 1.4358, Accuracy: 43.6250\n",
      "Epoch [83/100] Loss: 1.4402, Accuracy: 44.3750\n",
      "Epoch [84/100] Loss: 1.4161, Accuracy: 44.6250\n",
      "Epoch [85/100] Loss: 1.4061, Accuracy: 44.7500\n",
      "Epoch [86/100] Loss: 1.4067, Accuracy: 44.6250\n",
      "Epoch [87/100] Loss: 1.4040, Accuracy: 46.6250\n",
      "Epoch [88/100] Loss: 1.3844, Accuracy: 46.0000\n",
      "Epoch [89/100] Loss: 1.3833, Accuracy: 46.6250\n",
      "Epoch [90/100] Loss: 1.3810, Accuracy: 46.7500\n",
      "Epoch [91/100] Loss: 1.3620, Accuracy: 47.6250\n",
      "Epoch [92/100] Loss: 1.3628, Accuracy: 47.5000\n",
      "Epoch [93/100] Loss: 1.3571, Accuracy: 47.8750\n",
      "Epoch [94/100] Loss: 1.3563, Accuracy: 47.6250\n",
      "Epoch [95/100] Loss: 1.3494, Accuracy: 47.0000\n",
      "Epoch [96/100] Loss: 1.3377, Accuracy: 49.8750\n",
      "Epoch [97/100] Loss: 1.3300, Accuracy: 49.5000\n",
      "Epoch [98/100] Loss: 1.3175, Accuracy: 47.7500\n",
      "Epoch [99/100] Loss: 1.3127, Accuracy: 49.8750\n",
      "Epoch [100/100] Loss: 1.3151, Accuracy: 48.8750\n",
      "Validation:\n",
      ">> Fold [2/5]\n",
      "Training:\n",
      "Epoch [1/100] Loss: 1.7958, Accuracy: 17.1250\n",
      "Epoch [2/100] Loss: 1.7947, Accuracy: 17.1250\n",
      "Epoch [3/100] Loss: 1.7922, Accuracy: 17.1250\n",
      "Epoch [4/100] Loss: 1.7913, Accuracy: 17.1250\n",
      "Epoch [5/100] Loss: 1.7907, Accuracy: 19.5000\n",
      "Epoch [6/100] Loss: 1.7898, Accuracy: 18.7500\n",
      "Epoch [7/100] Loss: 1.7889, Accuracy: 21.3750\n",
      "Epoch [8/100] Loss: 1.7885, Accuracy: 20.8750\n",
      "Epoch [9/100] Loss: 1.7884, Accuracy: 20.0000\n",
      "Epoch [10/100] Loss: 1.7879, Accuracy: 20.6250\n",
      "Epoch [11/100] Loss: 1.7872, Accuracy: 21.7500\n",
      "Epoch [12/100] Loss: 1.7865, Accuracy: 23.1250\n",
      "Epoch [13/100] Loss: 1.7859, Accuracy: 23.6250\n",
      "Epoch [14/100] Loss: 1.7850, Accuracy: 23.3750\n",
      "Epoch [15/100] Loss: 1.7846, Accuracy: 21.0000\n",
      "Epoch [16/100] Loss: 1.7812, Accuracy: 21.0000\n",
      "Epoch [17/100] Loss: 1.7801, Accuracy: 23.8750\n",
      "Epoch [18/100] Loss: 1.7773, Accuracy: 23.2500\n",
      "Epoch [19/100] Loss: 1.7751, Accuracy: 23.8750\n",
      "Epoch [20/100] Loss: 1.7736, Accuracy: 23.5000\n",
      "Epoch [21/100] Loss: 1.7709, Accuracy: 22.6250\n",
      "Epoch [22/100] Loss: 1.7697, Accuracy: 22.1250\n",
      "Epoch [23/100] Loss: 1.7668, Accuracy: 22.0000\n",
      "Epoch [24/100] Loss: 1.7671, Accuracy: 23.6250\n",
      "Epoch [25/100] Loss: 1.7638, Accuracy: 22.8750\n",
      "Epoch [26/100] Loss: 1.7634, Accuracy: 22.2500\n",
      "Epoch [27/100] Loss: 1.7580, Accuracy: 23.6250\n",
      "Epoch [28/100] Loss: 1.7579, Accuracy: 24.6250\n",
      "Epoch [29/100] Loss: 1.7513, Accuracy: 24.2500\n",
      "Epoch [30/100] Loss: 1.7536, Accuracy: 25.1250\n",
      "Epoch [31/100] Loss: 1.7509, Accuracy: 24.8750\n",
      "Epoch [32/100] Loss: 1.7459, Accuracy: 24.7500\n",
      "Epoch [33/100] Loss: 1.7450, Accuracy: 25.3750\n",
      "Epoch [34/100] Loss: 1.7412, Accuracy: 25.7500\n",
      "Epoch [35/100] Loss: 1.7392, Accuracy: 27.3750\n",
      "Epoch [36/100] Loss: 1.7335, Accuracy: 27.2500\n",
      "Epoch [37/100] Loss: 1.7333, Accuracy: 28.5000\n",
      "Epoch [38/100] Loss: 1.7337, Accuracy: 27.8750\n",
      "Epoch [39/100] Loss: 1.7283, Accuracy: 26.6250\n",
      "Epoch [40/100] Loss: 1.7237, Accuracy: 26.7500\n",
      "Epoch [41/100] Loss: 1.7195, Accuracy: 27.0000\n",
      "Epoch [42/100] Loss: 1.7168, Accuracy: 27.1250\n",
      "Epoch [43/100] Loss: 1.7107, Accuracy: 27.6250\n",
      "Epoch [44/100] Loss: 1.7112, Accuracy: 27.8750\n",
      "Epoch [45/100] Loss: 1.7090, Accuracy: 27.3750\n",
      "Epoch [46/100] Loss: 1.6994, Accuracy: 27.2500\n",
      "Epoch [47/100] Loss: 1.6990, Accuracy: 28.8750\n",
      "Epoch [48/100] Loss: 1.6911, Accuracy: 30.1250\n",
      "Epoch [49/100] Loss: 1.6896, Accuracy: 30.6250\n",
      "Epoch [50/100] Loss: 1.6802, Accuracy: 30.7500\n",
      "Epoch [51/100] Loss: 1.6749, Accuracy: 31.1250\n",
      "Epoch [52/100] Loss: 1.6714, Accuracy: 30.7500\n",
      "Epoch [53/100] Loss: 1.6752, Accuracy: 28.8750\n",
      "Epoch [54/100] Loss: 1.6631, Accuracy: 30.7500\n",
      "Epoch [55/100] Loss: 1.6519, Accuracy: 32.1250\n",
      "Epoch [56/100] Loss: 1.6537, Accuracy: 31.5000\n",
      "Epoch [57/100] Loss: 1.6452, Accuracy: 32.5000\n",
      "Epoch [58/100] Loss: 1.6372, Accuracy: 32.0000\n",
      "Epoch [59/100] Loss: 1.6301, Accuracy: 33.1250\n",
      "Epoch [60/100] Loss: 1.6277, Accuracy: 32.3750\n",
      "Epoch [61/100] Loss: 1.6187, Accuracy: 33.7500\n",
      "Epoch [62/100] Loss: 1.6146, Accuracy: 33.1250\n",
      "Epoch [63/100] Loss: 1.6046, Accuracy: 34.0000\n",
      "Epoch [64/100] Loss: 1.5999, Accuracy: 35.1250\n",
      "Epoch [65/100] Loss: 1.5811, Accuracy: 36.7500\n",
      "Epoch [66/100] Loss: 1.5894, Accuracy: 36.2500\n",
      "Epoch [67/100] Loss: 1.5774, Accuracy: 34.5000\n",
      "Epoch [68/100] Loss: 1.5719, Accuracy: 37.2500\n",
      "Epoch [69/100] Loss: 1.5648, Accuracy: 35.7500\n",
      "Epoch [70/100] Loss: 1.5561, Accuracy: 36.2500\n",
      "Epoch [71/100] Loss: 1.5561, Accuracy: 36.8750\n",
      "Epoch [72/100] Loss: 1.5490, Accuracy: 38.6250\n",
      "Epoch [73/100] Loss: 1.5386, Accuracy: 38.0000\n",
      "Epoch [74/100] Loss: 1.5259, Accuracy: 38.2500\n",
      "Epoch [75/100] Loss: 1.5231, Accuracy: 39.5000\n",
      "Epoch [76/100] Loss: 1.5076, Accuracy: 40.2500\n",
      "Epoch [77/100] Loss: 1.5083, Accuracy: 41.1250\n",
      "Epoch [78/100] Loss: 1.4992, Accuracy: 40.8750\n",
      "Epoch [79/100] Loss: 1.4878, Accuracy: 41.8750\n",
      "Epoch [80/100] Loss: 1.4884, Accuracy: 40.8750\n",
      "Epoch [81/100] Loss: 1.4910, Accuracy: 40.3750\n",
      "Epoch [82/100] Loss: 1.4759, Accuracy: 41.6250\n",
      "Epoch [83/100] Loss: 1.4703, Accuracy: 42.2500\n",
      "Epoch [84/100] Loss: 1.4646, Accuracy: 42.3750\n",
      "Epoch [85/100] Loss: 1.4512, Accuracy: 42.1250\n",
      "Epoch [86/100] Loss: 1.4550, Accuracy: 43.5000\n",
      "Epoch [87/100] Loss: 1.4462, Accuracy: 41.8750\n",
      "Epoch [88/100] Loss: 1.4421, Accuracy: 42.1250\n",
      "Epoch [89/100] Loss: 1.4199, Accuracy: 43.8750\n",
      "Epoch [90/100] Loss: 1.4103, Accuracy: 45.0000\n",
      "Epoch [91/100] Loss: 1.4128, Accuracy: 45.6250\n",
      "Epoch [92/100] Loss: 1.4168, Accuracy: 43.7500\n",
      "Epoch [93/100] Loss: 1.4098, Accuracy: 43.0000\n",
      "Epoch [94/100] Loss: 1.3876, Accuracy: 45.2500\n",
      "Epoch [95/100] Loss: 1.4065, Accuracy: 43.5000\n",
      "Epoch [96/100] Loss: 1.3803, Accuracy: 45.1250\n",
      "Epoch [97/100] Loss: 1.3776, Accuracy: 44.7500\n",
      "Epoch [98/100] Loss: 1.3612, Accuracy: 47.1250\n",
      "Epoch [99/100] Loss: 1.3573, Accuracy: 46.5000\n",
      "Epoch [100/100] Loss: 1.3597, Accuracy: 47.7500\n",
      "Validation:\n",
      ">> Fold [3/5]\n",
      "Training:\n",
      "Epoch [1/100] Loss: 1.7997, Accuracy: 15.0000\n",
      "Epoch [2/100] Loss: 1.7932, Accuracy: 15.6250\n",
      "Epoch [3/100] Loss: 1.7899, Accuracy: 18.5000\n",
      "Epoch [4/100] Loss: 1.7875, Accuracy: 20.3750\n",
      "Epoch [5/100] Loss: 1.7869, Accuracy: 19.2500\n",
      "Epoch [6/100] Loss: 1.7868, Accuracy: 20.2500\n",
      "Epoch [7/100] Loss: 1.7873, Accuracy: 20.0000\n",
      "Epoch [8/100] Loss: 1.7853, Accuracy: 19.7500\n",
      "Epoch [9/100] Loss: 1.7851, Accuracy: 21.7500\n",
      "Epoch [10/100] Loss: 1.7841, Accuracy: 23.1250\n",
      "Epoch [11/100] Loss: 1.7823, Accuracy: 22.7500\n",
      "Epoch [12/100] Loss: 1.7810, Accuracy: 22.6250\n",
      "Epoch [13/100] Loss: 1.7788, Accuracy: 21.8750\n",
      "Epoch [14/100] Loss: 1.7761, Accuracy: 22.3750\n",
      "Epoch [15/100] Loss: 1.7748, Accuracy: 23.2500\n",
      "Epoch [16/100] Loss: 1.7729, Accuracy: 22.1250\n",
      "Epoch [17/100] Loss: 1.7714, Accuracy: 23.0000\n",
      "Epoch [18/100] Loss: 1.7683, Accuracy: 24.2500\n",
      "Epoch [19/100] Loss: 1.7676, Accuracy: 22.0000\n",
      "Epoch [20/100] Loss: 1.7626, Accuracy: 23.7500\n",
      "Epoch [21/100] Loss: 1.7600, Accuracy: 24.2500\n",
      "Epoch [22/100] Loss: 1.7608, Accuracy: 25.5000\n",
      "Epoch [23/100] Loss: 1.7540, Accuracy: 23.7500\n",
      "Epoch [24/100] Loss: 1.7510, Accuracy: 24.3750\n",
      "Epoch [25/100] Loss: 1.7486, Accuracy: 25.3750\n",
      "Epoch [26/100] Loss: 1.7472, Accuracy: 25.8750\n",
      "Epoch [27/100] Loss: 1.7408, Accuracy: 26.6250\n",
      "Epoch [28/100] Loss: 1.7389, Accuracy: 27.0000\n",
      "Epoch [29/100] Loss: 1.7325, Accuracy: 27.0000\n",
      "Epoch [30/100] Loss: 1.7324, Accuracy: 26.7500\n",
      "Epoch [31/100] Loss: 1.7322, Accuracy: 27.0000\n",
      "Epoch [32/100] Loss: 1.7259, Accuracy: 26.6250\n",
      "Epoch [33/100] Loss: 1.7387, Accuracy: 26.7500\n",
      "Epoch [34/100] Loss: 1.7283, Accuracy: 29.2500\n",
      "Epoch [35/100] Loss: 1.7175, Accuracy: 30.2500\n",
      "Epoch [36/100] Loss: 1.7122, Accuracy: 31.0000\n",
      "Epoch [37/100] Loss: 1.7126, Accuracy: 29.2500\n",
      "Epoch [38/100] Loss: 1.7110, Accuracy: 30.6250\n",
      "Epoch [39/100] Loss: 1.7070, Accuracy: 28.6250\n",
      "Epoch [40/100] Loss: 1.7084, Accuracy: 30.2500\n",
      "Epoch [41/100] Loss: 1.7012, Accuracy: 29.2500\n",
      "Epoch [42/100] Loss: 1.6910, Accuracy: 31.6250\n",
      "Epoch [43/100] Loss: 1.6943, Accuracy: 31.2500\n",
      "Epoch [44/100] Loss: 1.6874, Accuracy: 31.8750\n",
      "Epoch [45/100] Loss: 1.6778, Accuracy: 32.6250\n",
      "Epoch [46/100] Loss: 1.6758, Accuracy: 31.7500\n",
      "Epoch [47/100] Loss: 1.6706, Accuracy: 32.3750\n",
      "Epoch [48/100] Loss: 1.6674, Accuracy: 31.5000\n",
      "Epoch [49/100] Loss: 1.6649, Accuracy: 34.0000\n",
      "Epoch [50/100] Loss: 1.6591, Accuracy: 31.5000\n",
      "Epoch [51/100] Loss: 1.6489, Accuracy: 33.2500\n",
      "Epoch [52/100] Loss: 1.6447, Accuracy: 33.8750\n",
      "Epoch [53/100] Loss: 1.6363, Accuracy: 34.5000\n",
      "Epoch [54/100] Loss: 1.6404, Accuracy: 34.2500\n",
      "Epoch [55/100] Loss: 1.6420, Accuracy: 31.7500\n",
      "Epoch [56/100] Loss: 1.6246, Accuracy: 34.8750\n",
      "Epoch [57/100] Loss: 1.6294, Accuracy: 34.1250\n",
      "Epoch [58/100] Loss: 1.6098, Accuracy: 35.5000\n",
      "Epoch [59/100] Loss: 1.6258, Accuracy: 34.3750\n",
      "Epoch [60/100] Loss: 1.6054, Accuracy: 33.1250\n",
      "Epoch [61/100] Loss: 1.6385, Accuracy: 32.2500\n",
      "Epoch [62/100] Loss: 1.6054, Accuracy: 36.8750\n",
      "Epoch [63/100] Loss: 1.5813, Accuracy: 36.6250\n",
      "Epoch [64/100] Loss: 1.5788, Accuracy: 37.1250\n",
      "Epoch [65/100] Loss: 1.5834, Accuracy: 34.2500\n",
      "Epoch [66/100] Loss: 1.5923, Accuracy: 35.3750\n",
      "Epoch [67/100] Loss: 1.5638, Accuracy: 36.8750\n",
      "Epoch [68/100] Loss: 1.5567, Accuracy: 37.5000\n",
      "Epoch [69/100] Loss: 1.5522, Accuracy: 38.1250\n",
      "Epoch [70/100] Loss: 1.5569, Accuracy: 37.6250\n",
      "Epoch [71/100] Loss: 1.5421, Accuracy: 38.0000\n",
      "Epoch [72/100] Loss: 1.5319, Accuracy: 37.5000\n",
      "Epoch [73/100] Loss: 1.5406, Accuracy: 38.8750\n",
      "Epoch [74/100] Loss: 1.5280, Accuracy: 39.0000\n",
      "Epoch [75/100] Loss: 1.5157, Accuracy: 39.1250\n",
      "Epoch [76/100] Loss: 1.5265, Accuracy: 38.5000\n",
      "Epoch [77/100] Loss: 1.5177, Accuracy: 37.0000\n",
      "Epoch [78/100] Loss: 1.5001, Accuracy: 38.6250\n",
      "Epoch [79/100] Loss: 1.5040, Accuracy: 41.7500\n",
      "Epoch [80/100] Loss: 1.4913, Accuracy: 41.7500\n",
      "Epoch [81/100] Loss: 1.4875, Accuracy: 41.7500\n",
      "Epoch [82/100] Loss: 1.4776, Accuracy: 41.7500\n",
      "Epoch [83/100] Loss: 1.4679, Accuracy: 42.2500\n",
      "Epoch [84/100] Loss: 1.4665, Accuracy: 40.5000\n",
      "Epoch [85/100] Loss: 1.4516, Accuracy: 43.6250\n",
      "Epoch [86/100] Loss: 1.4455, Accuracy: 43.6250\n",
      "Epoch [87/100] Loss: 1.4382, Accuracy: 43.0000\n",
      "Epoch [88/100] Loss: 1.4361, Accuracy: 43.3750\n",
      "Epoch [89/100] Loss: 1.4464, Accuracy: 41.6250\n",
      "Epoch [90/100] Loss: 1.4245, Accuracy: 44.2500\n",
      "Epoch [91/100] Loss: 1.4125, Accuracy: 44.6250\n",
      "Epoch [92/100] Loss: 1.4193, Accuracy: 45.5000\n",
      "Epoch [93/100] Loss: 1.4250, Accuracy: 45.3750\n",
      "Epoch [94/100] Loss: 1.4093, Accuracy: 44.7500\n",
      "Epoch [95/100] Loss: 1.3926, Accuracy: 45.3750\n",
      "Epoch [96/100] Loss: 1.3841, Accuracy: 47.3750\n",
      "Epoch [97/100] Loss: 1.3900, Accuracy: 44.1250\n",
      "Epoch [98/100] Loss: 1.3851, Accuracy: 45.8750\n",
      "Epoch [99/100] Loss: 1.3601, Accuracy: 45.8750\n",
      "Epoch [100/100] Loss: 1.3761, Accuracy: 46.0000\n",
      "Validation:\n",
      ">> Fold [4/5]\n",
      "Training:\n",
      "Epoch [1/100] Loss: 1.7952, Accuracy: 15.5000\n",
      "Epoch [2/100] Loss: 1.7939, Accuracy: 16.8750\n",
      "Epoch [3/100] Loss: 1.7932, Accuracy: 18.2500\n",
      "Epoch [4/100] Loss: 1.7912, Accuracy: 18.2500\n",
      "Epoch [5/100] Loss: 1.7908, Accuracy: 18.2500\n",
      "Epoch [6/100] Loss: 1.7901, Accuracy: 18.2500\n",
      "Epoch [7/100] Loss: 1.7891, Accuracy: 18.2500\n",
      "Epoch [8/100] Loss: 1.7883, Accuracy: 18.2500\n",
      "Epoch [9/100] Loss: 1.7877, Accuracy: 19.0000\n",
      "Epoch [10/100] Loss: 1.7874, Accuracy: 18.5000\n",
      "Epoch [11/100] Loss: 1.7868, Accuracy: 18.1250\n",
      "Epoch [12/100] Loss: 1.7851, Accuracy: 18.5000\n",
      "Epoch [13/100] Loss: 1.7844, Accuracy: 18.3750\n",
      "Epoch [14/100] Loss: 1.7848, Accuracy: 19.1250\n",
      "Epoch [15/100] Loss: 1.7835, Accuracy: 20.8750\n",
      "Epoch [16/100] Loss: 1.7824, Accuracy: 19.1250\n",
      "Epoch [17/100] Loss: 1.7808, Accuracy: 18.8750\n",
      "Epoch [18/100] Loss: 1.7801, Accuracy: 21.7500\n",
      "Epoch [19/100] Loss: 1.7762, Accuracy: 20.0000\n",
      "Epoch [20/100] Loss: 1.7774, Accuracy: 21.7500\n",
      "Epoch [21/100] Loss: 1.7741, Accuracy: 21.5000\n",
      "Epoch [22/100] Loss: 1.7699, Accuracy: 22.3750\n",
      "Epoch [23/100] Loss: 1.7689, Accuracy: 21.3750\n",
      "Epoch [24/100] Loss: 1.7679, Accuracy: 21.5000\n",
      "Epoch [25/100] Loss: 1.7642, Accuracy: 22.8750\n",
      "Epoch [26/100] Loss: 1.7582, Accuracy: 23.2500\n",
      "Epoch [27/100] Loss: 1.7681, Accuracy: 21.7500\n",
      "Epoch [28/100] Loss: 1.7609, Accuracy: 23.0000\n",
      "Epoch [29/100] Loss: 1.7563, Accuracy: 22.7500\n",
      "Epoch [30/100] Loss: 1.7551, Accuracy: 23.3750\n",
      "Epoch [31/100] Loss: 1.7538, Accuracy: 23.6250\n",
      "Epoch [32/100] Loss: 1.7488, Accuracy: 24.0000\n",
      "Epoch [33/100] Loss: 1.7481, Accuracy: 24.2500\n",
      "Epoch [34/100] Loss: 1.7458, Accuracy: 24.3750\n",
      "Epoch [35/100] Loss: 1.7383, Accuracy: 25.7500\n",
      "Epoch [36/100] Loss: 1.7381, Accuracy: 25.2500\n",
      "Epoch [37/100] Loss: 1.7400, Accuracy: 24.6250\n",
      "Epoch [38/100] Loss: 1.7271, Accuracy: 25.8750\n",
      "Epoch [39/100] Loss: 1.7232, Accuracy: 27.1250\n",
      "Epoch [40/100] Loss: 1.7227, Accuracy: 27.3750\n",
      "Epoch [41/100] Loss: 1.7167, Accuracy: 26.8750\n",
      "Epoch [42/100] Loss: 1.7135, Accuracy: 28.3750\n",
      "Epoch [43/100] Loss: 1.7115, Accuracy: 27.7500\n",
      "Epoch [44/100] Loss: 1.7077, Accuracy: 29.1250\n",
      "Epoch [45/100] Loss: 1.7004, Accuracy: 28.7500\n",
      "Epoch [46/100] Loss: 1.7010, Accuracy: 28.1250\n",
      "Epoch [47/100] Loss: 1.6966, Accuracy: 29.8750\n",
      "Epoch [48/100] Loss: 1.6943, Accuracy: 28.2500\n",
      "Epoch [49/100] Loss: 1.6834, Accuracy: 30.1250\n",
      "Epoch [50/100] Loss: 1.6771, Accuracy: 28.8750\n",
      "Epoch [51/100] Loss: 1.6762, Accuracy: 30.0000\n",
      "Epoch [52/100] Loss: 1.6685, Accuracy: 31.1250\n",
      "Epoch [53/100] Loss: 1.6790, Accuracy: 29.3750\n",
      "Epoch [54/100] Loss: 1.6660, Accuracy: 30.7500\n",
      "Epoch [55/100] Loss: 1.6577, Accuracy: 32.2500\n",
      "Epoch [56/100] Loss: 1.6524, Accuracy: 31.8750\n",
      "Epoch [57/100] Loss: 1.6446, Accuracy: 32.6250\n",
      "Epoch [58/100] Loss: 1.6455, Accuracy: 32.3750\n",
      "Epoch [59/100] Loss: 1.6366, Accuracy: 32.3750\n",
      "Epoch [60/100] Loss: 1.6308, Accuracy: 30.7500\n",
      "Epoch [61/100] Loss: 1.6307, Accuracy: 32.6250\n",
      "Epoch [62/100] Loss: 1.6258, Accuracy: 34.7500\n",
      "Epoch [63/100] Loss: 1.6137, Accuracy: 33.7500\n",
      "Epoch [64/100] Loss: 1.6081, Accuracy: 35.1250\n",
      "Epoch [65/100] Loss: 1.6101, Accuracy: 33.1250\n",
      "Epoch [66/100] Loss: 1.6090, Accuracy: 32.8750\n",
      "Epoch [67/100] Loss: 1.6035, Accuracy: 35.1250\n",
      "Epoch [68/100] Loss: 1.5910, Accuracy: 36.1250\n",
      "Epoch [69/100] Loss: 1.5888, Accuracy: 35.8750\n",
      "Epoch [70/100] Loss: 1.5892, Accuracy: 35.5000\n",
      "Epoch [71/100] Loss: 1.5800, Accuracy: 35.6250\n",
      "Epoch [72/100] Loss: 1.5934, Accuracy: 35.3750\n",
      "Epoch [73/100] Loss: 1.5830, Accuracy: 35.5000\n",
      "Epoch [74/100] Loss: 1.5745, Accuracy: 35.1250\n",
      "Epoch [75/100] Loss: 1.5659, Accuracy: 36.8750\n",
      "Epoch [76/100] Loss: 1.5629, Accuracy: 36.2500\n",
      "Epoch [77/100] Loss: 1.5567, Accuracy: 37.0000\n",
      "Epoch [78/100] Loss: 1.5590, Accuracy: 35.7500\n",
      "Epoch [79/100] Loss: 1.5458, Accuracy: 37.3750\n",
      "Epoch [80/100] Loss: 1.5444, Accuracy: 37.1250\n",
      "Epoch [81/100] Loss: 1.5461, Accuracy: 38.5000\n",
      "Epoch [82/100] Loss: 1.5371, Accuracy: 39.1250\n",
      "Epoch [83/100] Loss: 1.5366, Accuracy: 39.0000\n",
      "Epoch [84/100] Loss: 1.5246, Accuracy: 39.2500\n",
      "Epoch [85/100] Loss: 1.5156, Accuracy: 38.0000\n",
      "Epoch [86/100] Loss: 1.5138, Accuracy: 39.3750\n",
      "Epoch [87/100] Loss: 1.5173, Accuracy: 40.6250\n",
      "Epoch [88/100] Loss: 1.5016, Accuracy: 40.1250\n",
      "Epoch [89/100] Loss: 1.5061, Accuracy: 40.2500\n",
      "Epoch [90/100] Loss: 1.4954, Accuracy: 41.0000\n",
      "Epoch [91/100] Loss: 1.4943, Accuracy: 41.8750\n",
      "Epoch [92/100] Loss: 1.4915, Accuracy: 41.3750\n",
      "Epoch [93/100] Loss: 1.4823, Accuracy: 41.6250\n",
      "Epoch [94/100] Loss: 1.4929, Accuracy: 39.1250\n",
      "Epoch [95/100] Loss: 1.5319, Accuracy: 38.3750\n",
      "Epoch [96/100] Loss: 1.5068, Accuracy: 40.8750\n",
      "Epoch [97/100] Loss: 1.4909, Accuracy: 43.1250\n",
      "Epoch [98/100] Loss: 1.4758, Accuracy: 43.8750\n",
      "Epoch [99/100] Loss: 1.4624, Accuracy: 42.7500\n",
      "Epoch [100/100] Loss: 1.4547, Accuracy: 44.1250\n",
      "Validation:\n",
      ">> Fold [5/5]\n",
      "Training:\n",
      "Epoch [1/100] Loss: 1.7916, Accuracy: 18.8750\n",
      "Epoch [2/100] Loss: 1.7901, Accuracy: 18.8750\n",
      "Epoch [3/100] Loss: 1.7884, Accuracy: 18.8750\n",
      "Epoch [4/100] Loss: 1.7863, Accuracy: 18.8750\n",
      "Epoch [5/100] Loss: 1.7861, Accuracy: 18.8750\n",
      "Epoch [6/100] Loss: 1.7873, Accuracy: 18.8750\n",
      "Epoch [7/100] Loss: 1.7851, Accuracy: 18.8750\n",
      "Epoch [8/100] Loss: 1.7845, Accuracy: 18.8750\n",
      "Epoch [9/100] Loss: 1.7850, Accuracy: 19.5000\n",
      "Epoch [10/100] Loss: 1.7851, Accuracy: 19.1250\n",
      "Epoch [11/100] Loss: 1.7825, Accuracy: 18.8750\n",
      "Epoch [12/100] Loss: 1.7795, Accuracy: 21.3750\n",
      "Epoch [13/100] Loss: 1.7788, Accuracy: 21.6250\n",
      "Epoch [14/100] Loss: 1.7759, Accuracy: 22.0000\n",
      "Epoch [15/100] Loss: 1.7728, Accuracy: 21.7500\n",
      "Epoch [16/100] Loss: 1.7703, Accuracy: 22.1250\n",
      "Epoch [17/100] Loss: 1.7682, Accuracy: 23.5000\n",
      "Epoch [18/100] Loss: 1.7640, Accuracy: 22.6250\n",
      "Epoch [19/100] Loss: 1.7610, Accuracy: 24.1250\n",
      "Epoch [20/100] Loss: 1.7583, Accuracy: 24.2500\n",
      "Epoch [21/100] Loss: 1.7579, Accuracy: 25.1250\n",
      "Epoch [22/100] Loss: 1.7544, Accuracy: 24.5000\n",
      "Epoch [23/100] Loss: 1.7509, Accuracy: 23.3750\n",
      "Epoch [24/100] Loss: 1.7478, Accuracy: 25.8750\n",
      "Epoch [25/100] Loss: 1.7445, Accuracy: 24.7500\n",
      "Epoch [26/100] Loss: 1.7340, Accuracy: 25.5000\n",
      "Epoch [27/100] Loss: 1.7360, Accuracy: 26.6250\n",
      "Epoch [28/100] Loss: 1.7324, Accuracy: 24.6250\n",
      "Epoch [29/100] Loss: 1.7271, Accuracy: 27.0000\n",
      "Epoch [30/100] Loss: 1.7247, Accuracy: 26.0000\n",
      "Epoch [31/100] Loss: 1.7275, Accuracy: 26.6250\n",
      "Epoch [32/100] Loss: 1.7182, Accuracy: 26.6250\n",
      "Epoch [33/100] Loss: 1.7164, Accuracy: 26.5000\n",
      "Epoch [34/100] Loss: 1.7112, Accuracy: 27.1250\n",
      "Epoch [35/100] Loss: 1.7197, Accuracy: 24.3750\n",
      "Epoch [36/100] Loss: 1.7044, Accuracy: 27.7500\n",
      "Epoch [37/100] Loss: 1.7091, Accuracy: 28.2500\n",
      "Epoch [38/100] Loss: 1.7201, Accuracy: 26.2500\n",
      "Epoch [39/100] Loss: 1.7107, Accuracy: 27.2500\n",
      "Epoch [40/100] Loss: 1.6976, Accuracy: 29.2500\n",
      "Epoch [41/100] Loss: 1.6933, Accuracy: 28.1250\n",
      "Epoch [42/100] Loss: 1.6884, Accuracy: 27.6250\n",
      "Epoch [43/100] Loss: 1.6868, Accuracy: 28.6250\n",
      "Epoch [44/100] Loss: 1.6755, Accuracy: 28.8750\n",
      "Epoch [45/100] Loss: 1.6755, Accuracy: 29.7500\n",
      "Epoch [46/100] Loss: 1.6694, Accuracy: 30.3750\n",
      "Epoch [47/100] Loss: 1.6698, Accuracy: 29.3750\n",
      "Epoch [48/100] Loss: 1.6658, Accuracy: 29.0000\n",
      "Epoch [49/100] Loss: 1.6608, Accuracy: 30.7500\n",
      "Epoch [50/100] Loss: 1.6587, Accuracy: 31.1250\n",
      "Epoch [51/100] Loss: 1.6452, Accuracy: 30.8750\n",
      "Epoch [52/100] Loss: 1.6534, Accuracy: 30.6250\n",
      "Epoch [53/100] Loss: 1.6479, Accuracy: 32.1250\n",
      "Epoch [54/100] Loss: 1.6333, Accuracy: 32.7500\n",
      "Epoch [55/100] Loss: 1.6438, Accuracy: 32.1250\n",
      "Epoch [56/100] Loss: 1.6311, Accuracy: 33.1250\n",
      "Epoch [57/100] Loss: 1.6149, Accuracy: 34.5000\n",
      "Epoch [58/100] Loss: 1.6218, Accuracy: 33.5000\n",
      "Epoch [59/100] Loss: 1.6192, Accuracy: 33.0000\n",
      "Epoch [60/100] Loss: 1.6107, Accuracy: 33.1250\n",
      "Epoch [61/100] Loss: 1.5972, Accuracy: 35.2500\n",
      "Epoch [62/100] Loss: 1.5965, Accuracy: 36.3750\n",
      "Epoch [63/100] Loss: 1.5937, Accuracy: 35.8750\n",
      "Epoch [64/100] Loss: 1.5843, Accuracy: 35.8750\n",
      "Epoch [65/100] Loss: 1.5726, Accuracy: 37.0000\n",
      "Epoch [66/100] Loss: 1.5922, Accuracy: 34.2500\n",
      "Epoch [67/100] Loss: 1.5725, Accuracy: 35.6250\n",
      "Epoch [68/100] Loss: 1.5570, Accuracy: 37.1250\n",
      "Epoch [69/100] Loss: 1.5578, Accuracy: 38.6250\n",
      "Epoch [70/100] Loss: 1.5527, Accuracy: 37.2500\n",
      "Epoch [71/100] Loss: 1.5563, Accuracy: 38.0000\n",
      "Epoch [72/100] Loss: 1.5544, Accuracy: 36.5000\n",
      "Epoch [73/100] Loss: 1.5462, Accuracy: 37.1250\n",
      "Epoch [74/100] Loss: 1.5443, Accuracy: 38.0000\n",
      "Epoch [75/100] Loss: 1.5331, Accuracy: 37.6250\n",
      "Epoch [76/100] Loss: 1.5319, Accuracy: 38.1250\n",
      "Epoch [77/100] Loss: 1.5191, Accuracy: 39.6250\n",
      "Epoch [78/100] Loss: 1.5250, Accuracy: 37.7500\n",
      "Epoch [79/100] Loss: 1.5244, Accuracy: 39.5000\n",
      "Epoch [80/100] Loss: 1.4993, Accuracy: 40.0000\n",
      "Epoch [81/100] Loss: 1.4981, Accuracy: 40.1250\n",
      "Epoch [82/100] Loss: 1.4947, Accuracy: 40.1250\n",
      "Epoch [83/100] Loss: 1.5018, Accuracy: 40.3750\n",
      "Epoch [84/100] Loss: 1.4906, Accuracy: 41.2500\n",
      "Epoch [85/100] Loss: 1.4803, Accuracy: 41.2500\n",
      "Epoch [86/100] Loss: 1.4733, Accuracy: 41.3750\n",
      "Epoch [87/100] Loss: 1.4767, Accuracy: 41.6250\n",
      "Epoch [88/100] Loss: 1.4642, Accuracy: 42.6250\n",
      "Epoch [89/100] Loss: 1.4580, Accuracy: 40.5000\n",
      "Epoch [90/100] Loss: 1.4665, Accuracy: 42.0000\n",
      "Epoch [91/100] Loss: 1.4568, Accuracy: 42.1250\n",
      "Epoch [92/100] Loss: 1.4451, Accuracy: 41.5000\n",
      "Epoch [93/100] Loss: 1.4433, Accuracy: 41.2500\n",
      "Epoch [94/100] Loss: 1.4443, Accuracy: 42.3750\n",
      "Epoch [95/100] Loss: 1.4360, Accuracy: 43.7500\n",
      "Epoch [96/100] Loss: 1.4307, Accuracy: 42.5000\n",
      "Epoch [97/100] Loss: 1.4336, Accuracy: 44.2500\n",
      "Epoch [98/100] Loss: 1.4142, Accuracy: 45.0000\n",
      "Epoch [99/100] Loss: 1.4110, Accuracy: 45.0000\n",
      "Epoch [100/100] Loss: 1.4227, Accuracy: 43.7500\n",
      "Validation:\n",
      "Cross-validation completed.\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Cross-validation:\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "    print(f\">> Fold [{fold + 1}/{k}]\")\n",
    "    print(\"Training:\")\n",
    "\n",
    "    fold_train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(train_idx))\n",
    "    fold_val_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(val_idx))\n",
    "\n",
    "    # Training\n",
    "    model = LateFusionEmotionClassifier(input_dim=18, output_dim=6).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss, epoch_accuracy = train(model, device, criterion, fold_train_loader, optimizer)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    print(\"Validation:\")\n",
    "    true_labels, pred_labels = validate(model, device, fold_val_loader)\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Cross-validation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Epoch [1/200] Loss: 1.7951, Accuracy: 17.7000\n",
      "Epoch [2/200] Loss: 1.7922, Accuracy: 17.7000\n",
      "Epoch [3/200] Loss: 1.7914, Accuracy: 17.7000\n",
      "Epoch [4/200] Loss: 1.7904, Accuracy: 17.7000\n",
      "Epoch [5/200] Loss: 1.7898, Accuracy: 17.8000\n",
      "Epoch [6/200] Loss: 1.7892, Accuracy: 18.3000\n",
      "Epoch [7/200] Loss: 1.7883, Accuracy: 18.7000\n",
      "Epoch [8/200] Loss: 1.7875, Accuracy: 21.0000\n",
      "Epoch [9/200] Loss: 1.7857, Accuracy: 19.3000\n",
      "Epoch [10/200] Loss: 1.7843, Accuracy: 21.3000\n",
      "Epoch [11/200] Loss: 1.7838, Accuracy: 21.6000\n",
      "Epoch [12/200] Loss: 1.7818, Accuracy: 20.1000\n",
      "Epoch [13/200] Loss: 1.7801, Accuracy: 21.2000\n",
      "Epoch [14/200] Loss: 1.7772, Accuracy: 21.2000\n",
      "Epoch [15/200] Loss: 1.7761, Accuracy: 22.0000\n",
      "Epoch [16/200] Loss: 1.7751, Accuracy: 20.9000\n",
      "Epoch [17/200] Loss: 1.7726, Accuracy: 22.4000\n",
      "Epoch [18/200] Loss: 1.7725, Accuracy: 22.2000\n",
      "Epoch [19/200] Loss: 1.7686, Accuracy: 21.9000\n",
      "Epoch [20/200] Loss: 1.7688, Accuracy: 23.6000\n",
      "Epoch [21/200] Loss: 1.7663, Accuracy: 22.9000\n",
      "Epoch [22/200] Loss: 1.7630, Accuracy: 23.3000\n",
      "Epoch [23/200] Loss: 1.7633, Accuracy: 23.0000\n",
      "Epoch [24/200] Loss: 1.7589, Accuracy: 25.0000\n",
      "Epoch [25/200] Loss: 1.7581, Accuracy: 23.4000\n",
      "Epoch [26/200] Loss: 1.7582, Accuracy: 23.3000\n",
      "Epoch [27/200] Loss: 1.7519, Accuracy: 24.0000\n",
      "Epoch [28/200] Loss: 1.7578, Accuracy: 24.9000\n",
      "Epoch [29/200] Loss: 1.7490, Accuracy: 23.5000\n",
      "Epoch [30/200] Loss: 1.7515, Accuracy: 23.0000\n",
      "Epoch [31/200] Loss: 1.7445, Accuracy: 25.4000\n",
      "Epoch [32/200] Loss: 1.7379, Accuracy: 25.3000\n",
      "Epoch [33/200] Loss: 1.7370, Accuracy: 25.4000\n",
      "Epoch [34/200] Loss: 1.7352, Accuracy: 24.9000\n",
      "Epoch [35/200] Loss: 1.7327, Accuracy: 25.9000\n",
      "Epoch [36/200] Loss: 1.7304, Accuracy: 27.1000\n",
      "Epoch [37/200] Loss: 1.7321, Accuracy: 25.2000\n",
      "Epoch [38/200] Loss: 1.7259, Accuracy: 26.5000\n",
      "Epoch [39/200] Loss: 1.7209, Accuracy: 26.5000\n",
      "Epoch [40/200] Loss: 1.7166, Accuracy: 27.1000\n",
      "Epoch [41/200] Loss: 1.7154, Accuracy: 27.0000\n",
      "Epoch [42/200] Loss: 1.7092, Accuracy: 27.3000\n",
      "Epoch [43/200] Loss: 1.7051, Accuracy: 28.0000\n",
      "Epoch [44/200] Loss: 1.7025, Accuracy: 27.4000\n",
      "Epoch [45/200] Loss: 1.6992, Accuracy: 29.0000\n",
      "Epoch [46/200] Loss: 1.6968, Accuracy: 28.7000\n",
      "Epoch [47/200] Loss: 1.6922, Accuracy: 29.7000\n",
      "Epoch [48/200] Loss: 1.6878, Accuracy: 30.1000\n",
      "Epoch [49/200] Loss: 1.6818, Accuracy: 29.0000\n",
      "Epoch [50/200] Loss: 1.6742, Accuracy: 29.1000\n",
      "Epoch [51/200] Loss: 1.6770, Accuracy: 28.5000\n",
      "Epoch [52/200] Loss: 1.6699, Accuracy: 29.8000\n",
      "Epoch [53/200] Loss: 1.6669, Accuracy: 29.9000\n",
      "Epoch [54/200] Loss: 1.6633, Accuracy: 29.8000\n",
      "Epoch [55/200] Loss: 1.6619, Accuracy: 30.5000\n",
      "Epoch [56/200] Loss: 1.6509, Accuracy: 31.1000\n",
      "Epoch [57/200] Loss: 1.6636, Accuracy: 29.8000\n",
      "Epoch [58/200] Loss: 1.6425, Accuracy: 32.1000\n",
      "Epoch [59/200] Loss: 1.6393, Accuracy: 32.2000\n",
      "Epoch [60/200] Loss: 1.6299, Accuracy: 31.6000\n",
      "Epoch [61/200] Loss: 1.6330, Accuracy: 32.8000\n",
      "Epoch [62/200] Loss: 1.6287, Accuracy: 32.4000\n",
      "Epoch [63/200] Loss: 1.6375, Accuracy: 31.8000\n",
      "Epoch [64/200] Loss: 1.6210, Accuracy: 33.5000\n",
      "Epoch [65/200] Loss: 1.6129, Accuracy: 34.2000\n",
      "Epoch [66/200] Loss: 1.6156, Accuracy: 32.6000\n",
      "Epoch [67/200] Loss: 1.6074, Accuracy: 32.7000\n",
      "Epoch [68/200] Loss: 1.6043, Accuracy: 34.0000\n",
      "Epoch [69/200] Loss: 1.6044, Accuracy: 33.2000\n",
      "Epoch [70/200] Loss: 1.5885, Accuracy: 34.8000\n",
      "Epoch [71/200] Loss: 1.5866, Accuracy: 34.3000\n",
      "Epoch [72/200] Loss: 1.5856, Accuracy: 34.6000\n",
      "Epoch [73/200] Loss: 1.5703, Accuracy: 36.4000\n",
      "Epoch [74/200] Loss: 1.5738, Accuracy: 36.1000\n",
      "Epoch [75/200] Loss: 1.5650, Accuracy: 34.3000\n",
      "Epoch [76/200] Loss: 1.5698, Accuracy: 36.7000\n",
      "Epoch [77/200] Loss: 1.5643, Accuracy: 35.5000\n",
      "Epoch [78/200] Loss: 1.5531, Accuracy: 36.4000\n",
      "Epoch [79/200] Loss: 1.5523, Accuracy: 36.4000\n",
      "Epoch [80/200] Loss: 1.5441, Accuracy: 37.5000\n",
      "Epoch [81/200] Loss: 1.5409, Accuracy: 38.5000\n",
      "Epoch [82/200] Loss: 1.5520, Accuracy: 35.9000\n",
      "Epoch [83/200] Loss: 1.5272, Accuracy: 36.6000\n",
      "Epoch [84/200] Loss: 1.5329, Accuracy: 37.1000\n",
      "Epoch [85/200] Loss: 1.5203, Accuracy: 38.4000\n",
      "Epoch [86/200] Loss: 1.5135, Accuracy: 37.9000\n",
      "Epoch [87/200] Loss: 1.5094, Accuracy: 37.3000\n",
      "Epoch [88/200] Loss: 1.5084, Accuracy: 40.0000\n",
      "Epoch [89/200] Loss: 1.5057, Accuracy: 38.4000\n",
      "Epoch [90/200] Loss: 1.4859, Accuracy: 39.8000\n",
      "Epoch [91/200] Loss: 1.4864, Accuracy: 40.1000\n",
      "Epoch [92/200] Loss: 1.4886, Accuracy: 41.2000\n",
      "Epoch [93/200] Loss: 1.4894, Accuracy: 39.7000\n",
      "Epoch [94/200] Loss: 1.4685, Accuracy: 40.9000\n",
      "Epoch [95/200] Loss: 1.4763, Accuracy: 39.9000\n",
      "Epoch [96/200] Loss: 1.4711, Accuracy: 40.9000\n",
      "Epoch [97/200] Loss: 1.4592, Accuracy: 41.4000\n",
      "Epoch [98/200] Loss: 1.4463, Accuracy: 41.6000\n",
      "Epoch [99/200] Loss: 1.4444, Accuracy: 42.9000\n",
      "Epoch [100/200] Loss: 1.4583, Accuracy: 42.5000\n",
      "Epoch [101/200] Loss: 1.4446, Accuracy: 42.1000\n",
      "Epoch [102/200] Loss: 1.4344, Accuracy: 42.0000\n",
      "Epoch [103/200] Loss: 1.4280, Accuracy: 43.0000\n",
      "Epoch [104/200] Loss: 1.4208, Accuracy: 42.7000\n",
      "Epoch [105/200] Loss: 1.4109, Accuracy: 43.4000\n",
      "Epoch [106/200] Loss: 1.4089, Accuracy: 42.4000\n",
      "Epoch [107/200] Loss: 1.4128, Accuracy: 44.5000\n",
      "Epoch [108/200] Loss: 1.4371, Accuracy: 42.0000\n",
      "Epoch [109/200] Loss: 1.3952, Accuracy: 45.5000\n",
      "Epoch [110/200] Loss: 1.3883, Accuracy: 43.7000\n",
      "Epoch [111/200] Loss: 1.3848, Accuracy: 43.9000\n",
      "Epoch [112/200] Loss: 1.3895, Accuracy: 44.5000\n",
      "Epoch [113/200] Loss: 1.3779, Accuracy: 46.1000\n",
      "Epoch [114/200] Loss: 1.3868, Accuracy: 44.3000\n",
      "Epoch [115/200] Loss: 1.3739, Accuracy: 45.9000\n",
      "Epoch [116/200] Loss: 1.3584, Accuracy: 45.6000\n",
      "Epoch [117/200] Loss: 1.3573, Accuracy: 45.4000\n",
      "Epoch [118/200] Loss: 1.3603, Accuracy: 47.1000\n",
      "Epoch [119/200] Loss: 1.3548, Accuracy: 46.0000\n",
      "Epoch [120/200] Loss: 1.3533, Accuracy: 45.4000\n",
      "Epoch [121/200] Loss: 1.3418, Accuracy: 47.7000\n",
      "Epoch [122/200] Loss: 1.3295, Accuracy: 48.2000\n",
      "Epoch [123/200] Loss: 1.3408, Accuracy: 47.8000\n",
      "Epoch [124/200] Loss: 1.3228, Accuracy: 47.8000\n",
      "Epoch [125/200] Loss: 1.3226, Accuracy: 48.8000\n",
      "Epoch [126/200] Loss: 1.3110, Accuracy: 48.4000\n",
      "Epoch [127/200] Loss: 1.3165, Accuracy: 48.2000\n",
      "Epoch [128/200] Loss: 1.3223, Accuracy: 47.6000\n",
      "Epoch [129/200] Loss: 1.3043, Accuracy: 48.9000\n",
      "Epoch [130/200] Loss: 1.3044, Accuracy: 49.6000\n",
      "Epoch [131/200] Loss: 1.2911, Accuracy: 50.8000\n",
      "Epoch [132/200] Loss: 1.2749, Accuracy: 51.1000\n",
      "Epoch [133/200] Loss: 1.2815, Accuracy: 50.5000\n",
      "Epoch [134/200] Loss: 1.2816, Accuracy: 50.5000\n",
      "Epoch [135/200] Loss: 1.2702, Accuracy: 51.5000\n",
      "Epoch [136/200] Loss: 1.2869, Accuracy: 48.2000\n",
      "Epoch [137/200] Loss: 1.2667, Accuracy: 50.9000\n",
      "Epoch [138/200] Loss: 1.2705, Accuracy: 53.7000\n",
      "Epoch [139/200] Loss: 1.2743, Accuracy: 49.4000\n",
      "Epoch [140/200] Loss: 1.2849, Accuracy: 48.8000\n",
      "Epoch [141/200] Loss: 1.2692, Accuracy: 50.2000\n",
      "Epoch [142/200] Loss: 1.2426, Accuracy: 52.6000\n",
      "Epoch [143/200] Loss: 1.2471, Accuracy: 52.1000\n",
      "Epoch [144/200] Loss: 1.2269, Accuracy: 53.6000\n",
      "Epoch [145/200] Loss: 1.2174, Accuracy: 54.0000\n",
      "Epoch [146/200] Loss: 1.2070, Accuracy: 54.5000\n",
      "Epoch [147/200] Loss: 1.2018, Accuracy: 53.9000\n",
      "Epoch [148/200] Loss: 1.2092, Accuracy: 54.9000\n",
      "Epoch [149/200] Loss: 1.2130, Accuracy: 54.5000\n",
      "Epoch [150/200] Loss: 1.2127, Accuracy: 52.9000\n",
      "Epoch [151/200] Loss: 1.2159, Accuracy: 52.7000\n",
      "Epoch [152/200] Loss: 1.2172, Accuracy: 53.4000\n",
      "Epoch [153/200] Loss: 1.1995, Accuracy: 53.8000\n",
      "Epoch [154/200] Loss: 1.1709, Accuracy: 56.4000\n",
      "Epoch [155/200] Loss: 1.1657, Accuracy: 55.5000\n",
      "Epoch [156/200] Loss: 1.1606, Accuracy: 56.2000\n",
      "Epoch [157/200] Loss: 1.1592, Accuracy: 56.2000\n",
      "Epoch [158/200] Loss: 1.1498, Accuracy: 57.3000\n",
      "Epoch [159/200] Loss: 1.1435, Accuracy: 57.1000\n",
      "Epoch [160/200] Loss: 1.1468, Accuracy: 56.8000\n",
      "Epoch [161/200] Loss: 1.1366, Accuracy: 55.5000\n",
      "Epoch [162/200] Loss: 1.1445, Accuracy: 57.3000\n",
      "Epoch [163/200] Loss: 1.1309, Accuracy: 57.5000\n",
      "Epoch [164/200] Loss: 1.1473, Accuracy: 57.6000\n",
      "Epoch [165/200] Loss: 1.1345, Accuracy: 57.4000\n",
      "Epoch [166/200] Loss: 1.1219, Accuracy: 56.9000\n",
      "Epoch [167/200] Loss: 1.1167, Accuracy: 57.8000\n",
      "Epoch [168/200] Loss: 1.1023, Accuracy: 58.8000\n",
      "Epoch [169/200] Loss: 1.1069, Accuracy: 57.8000\n",
      "Epoch [170/200] Loss: 1.1032, Accuracy: 58.5000\n",
      "Epoch [171/200] Loss: 1.0961, Accuracy: 59.5000\n",
      "Epoch [172/200] Loss: 1.0883, Accuracy: 59.9000\n",
      "Epoch [173/200] Loss: 1.0789, Accuracy: 61.2000\n",
      "Epoch [174/200] Loss: 1.0657, Accuracy: 60.8000\n",
      "Epoch [175/200] Loss: 1.0799, Accuracy: 60.7000\n",
      "Epoch [176/200] Loss: 1.0803, Accuracy: 59.9000\n",
      "Epoch [177/200] Loss: 1.0716, Accuracy: 58.7000\n",
      "Epoch [178/200] Loss: 1.0796, Accuracy: 58.7000\n",
      "Epoch [179/200] Loss: 1.0800, Accuracy: 58.5000\n",
      "Epoch [180/200] Loss: 1.0629, Accuracy: 60.6000\n",
      "Epoch [181/200] Loss: 1.0589, Accuracy: 60.9000\n",
      "Epoch [182/200] Loss: 1.0341, Accuracy: 61.5000\n",
      "Epoch [183/200] Loss: 1.0373, Accuracy: 62.2000\n",
      "Epoch [184/200] Loss: 1.0436, Accuracy: 61.2000\n",
      "Epoch [185/200] Loss: 1.0503, Accuracy: 62.3000\n",
      "Epoch [186/200] Loss: 1.0526, Accuracy: 59.8000\n",
      "Epoch [187/200] Loss: 1.0158, Accuracy: 62.6000\n",
      "Epoch [188/200] Loss: 1.0049, Accuracy: 62.7000\n",
      "Epoch [189/200] Loss: 1.0107, Accuracy: 63.1000\n",
      "Epoch [190/200] Loss: 1.0123, Accuracy: 62.9000\n",
      "Epoch [191/200] Loss: 1.0135, Accuracy: 61.6000\n",
      "Epoch [192/200] Loss: 1.0017, Accuracy: 64.0000\n",
      "Epoch [193/200] Loss: 0.9826, Accuracy: 64.3000\n",
      "Epoch [194/200] Loss: 0.9951, Accuracy: 63.7000\n",
      "Epoch [195/200] Loss: 0.9749, Accuracy: 64.6000\n",
      "Epoch [196/200] Loss: 1.0001, Accuracy: 63.0000\n",
      "Epoch [197/200] Loss: 0.9778, Accuracy: 64.8000\n",
      "Epoch [198/200] Loss: 0.9671, Accuracy: 64.7000\n",
      "Epoch [199/200] Loss: 0.9744, Accuracy: 65.3000\n",
      "Epoch [200/200] Loss: 1.0113, Accuracy: 62.2000\n",
      "Reached final epoch. Saving model.\n",
      "Training completed. Final model path: ./saved_models/multimodal_emotion_recognition_model_199_final.pth\n"
     ]
    }
   ],
   "source": [
    "model = LateFusionEmotionClassifier(input_dim=18, output_dim=6).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 200\n",
    "\n",
    "print(\"Training:\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss, epoch_accuracy = train(model, device, criterion, train_loader, optimizer)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    # Early termination\n",
    "    if epoch_accuracy > 70:\n",
    "        print(\"Early termination. Saving model.\")\n",
    "        model_path = f'./saved_models/multimodal_emotion_recognition_model_{epoch}_early.pth'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        break\n",
    "\n",
    "    # Final epoch\n",
    "    if epoch == num_epochs - 1:\n",
    "        print(\"Reached final epoch. Saving model.\")\n",
    "        model_path = f'./saved_models/multimodal_emotion_recognition_model_{epoch}_final.pth'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        break\n",
    "\n",
    "    # Save model periodically\n",
    "    interval = 10\n",
    "    if epoch % interval == 0:\n",
    "        model_path = f'./saved_models/multimodal_emotion_recognition_model_{epoch}_intermediate.pth'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f'Training completed. Final model path: {model_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.24      0.36      0.29        33\n",
      "     disgust       0.26      0.26      0.26        34\n",
      "     fearful       0.15      0.12      0.14        32\n",
      "       happy       0.09      0.10      0.09        31\n",
      "     neutral       0.27      0.19      0.22        32\n",
      "         sad       0.13      0.11      0.12        38\n",
      "\n",
      "    accuracy                           0.19       200\n",
      "   macro avg       0.19      0.19      0.19       200\n",
      "weighted avg       0.19      0.19      0.19       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1072cf4f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Validation:\")\n",
    "true_labels, pred_labels = validate(model, device, val_loader)\n",
    "\n",
    "# Performance metrics report\n",
    "label_mapping = { 0: 'angry', 1: 'disgust', 2: 'fearful', 3: 'happy', 4: 'neutral', 5: 'sad' }\n",
    "print(classification_report(true_labels, pred_labels, labels=list(label_mapping.keys()), target_names=list(label_mapping.values())))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels, labels=list(label_mapping.keys()))\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1, keepdims=True)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=label_mapping.values())\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Predictions vs True Labels ---\n",
      "Sample 000: True = neutral, Pred = disgust\n",
      "Sample 001: True = neutral, Pred = sad\n",
      "Sample 002: True = sad, Pred = fearful\n",
      "Sample 003: True = neutral, Pred = disgust\n",
      "Sample 004: True = angry, Pred = happy\n",
      "Sample 005: True = neutral, Pred = neutral\n",
      "Sample 006: True = neutral, Pred = disgust\n",
      "Sample 007: True = fearful, Pred = disgust\n",
      "Sample 008: True = happy, Pred = sad\n",
      "Sample 009: True = fearful, Pred = happy\n",
      "Sample 010: True = angry, Pred = fearful\n",
      "Sample 011: True = happy, Pred = neutral\n",
      "Sample 012: True = fearful, Pred = sad\n",
      "Sample 013: True = angry, Pred = happy\n",
      "Sample 014: True = fearful, Pred = disgust\n",
      "Sample 015: True = disgust, Pred = happy\n",
      "Sample 016: True = disgust, Pred = disgust\n",
      "Sample 017: True = angry, Pred = angry\n",
      "Sample 018: True = fearful, Pred = sad\n",
      "Sample 019: True = disgust, Pred = fearful\n",
      "Sample 020: True = disgust, Pred = disgust\n",
      "Sample 021: True = sad, Pred = fearful\n",
      "Sample 022: True = fearful, Pred = happy\n",
      "Sample 023: True = angry, Pred = angry\n",
      "Sample 024: True = sad, Pred = happy\n",
      "Sample 025: True = sad, Pred = neutral\n",
      "Sample 026: True = happy, Pred = angry\n",
      "Sample 027: True = happy, Pred = sad\n",
      "Sample 028: True = sad, Pred = angry\n",
      "Sample 029: True = disgust, Pred = neutral\n",
      "Sample 030: True = disgust, Pred = disgust\n",
      "Sample 031: True = sad, Pred = sad\n",
      "Sample 032: True = sad, Pred = neutral\n",
      "Sample 033: True = happy, Pred = disgust\n",
      "Sample 034: True = sad, Pred = disgust\n",
      "Sample 035: True = neutral, Pred = disgust\n",
      "Sample 036: True = angry, Pred = sad\n",
      "Sample 037: True = fearful, Pred = fearful\n",
      "Sample 038: True = neutral, Pred = fearful\n",
      "Sample 039: True = happy, Pred = fearful\n",
      "Sample 040: True = neutral, Pred = fearful\n",
      "Sample 041: True = angry, Pred = fearful\n",
      "Sample 042: True = neutral, Pred = sad\n",
      "Sample 043: True = fearful, Pred = disgust\n",
      "Sample 044: True = happy, Pred = angry\n",
      "Sample 045: True = neutral, Pred = disgust\n",
      "Sample 046: True = sad, Pred = happy\n",
      "Sample 047: True = fearful, Pred = neutral\n",
      "Sample 048: True = angry, Pred = angry\n",
      "Sample 049: True = angry, Pred = angry\n",
      "Sample 050: True = happy, Pred = sad\n",
      "Sample 051: True = happy, Pred = angry\n",
      "Sample 052: True = fearful, Pred = fearful\n",
      "Sample 053: True = sad, Pred = angry\n",
      "Sample 054: True = disgust, Pred = happy\n",
      "Sample 055: True = happy, Pred = happy\n",
      "Sample 056: True = sad, Pred = disgust\n",
      "Sample 057: True = happy, Pred = angry\n",
      "Sample 058: True = happy, Pred = fearful\n",
      "Sample 059: True = angry, Pred = happy\n",
      "Sample 060: True = fearful, Pred = neutral\n",
      "Sample 061: True = neutral, Pred = sad\n",
      "Sample 062: True = fearful, Pred = disgust\n",
      "Sample 063: True = sad, Pred = happy\n",
      "Sample 064: True = sad, Pred = angry\n",
      "Sample 065: True = sad, Pred = angry\n",
      "Sample 066: True = disgust, Pred = sad\n",
      "Sample 067: True = disgust, Pred = angry\n",
      "Sample 068: True = disgust, Pred = happy\n",
      "Sample 069: True = angry, Pred = fearful\n",
      "Sample 070: True = angry, Pred = happy\n",
      "Sample 071: True = fearful, Pred = disgust\n",
      "Sample 072: True = happy, Pred = angry\n",
      "Sample 073: True = angry, Pred = happy\n",
      "Sample 074: True = neutral, Pred = fearful\n",
      "Sample 075: True = neutral, Pred = sad\n",
      "Sample 076: True = fearful, Pred = sad\n",
      "Sample 077: True = angry, Pred = angry\n",
      "Sample 078: True = angry, Pred = angry\n",
      "Sample 079: True = fearful, Pred = angry\n",
      "Sample 080: True = angry, Pred = happy\n",
      "Sample 081: True = fearful, Pred = disgust\n",
      "Sample 082: True = disgust, Pred = angry\n",
      "Sample 083: True = fearful, Pred = fearful\n",
      "Sample 084: True = angry, Pred = angry\n",
      "Sample 085: True = disgust, Pred = angry\n",
      "Sample 086: True = sad, Pred = happy\n",
      "Sample 087: True = disgust, Pred = angry\n",
      "Sample 088: True = neutral, Pred = angry\n",
      "Sample 089: True = happy, Pred = angry\n",
      "Sample 090: True = sad, Pred = disgust\n",
      "Sample 091: True = happy, Pred = happy\n",
      "Sample 092: True = angry, Pred = angry\n",
      "Sample 093: True = neutral, Pred = angry\n",
      "Sample 094: True = fearful, Pred = neutral\n",
      "Sample 095: True = happy, Pred = sad\n",
      "Sample 096: True = neutral, Pred = fearful\n",
      "Sample 097: True = happy, Pred = fearful\n",
      "Sample 098: True = disgust, Pred = fearful\n",
      "Sample 099: True = disgust, Pred = disgust\n",
      "Sample 100: True = happy, Pred = fearful\n",
      "Sample 101: True = sad, Pred = sad\n",
      "Sample 102: True = angry, Pred = sad\n",
      "Sample 103: True = fearful, Pred = sad\n",
      "Sample 104: True = angry, Pred = disgust\n",
      "Sample 105: True = sad, Pred = neutral\n",
      "Sample 106: True = sad, Pred = neutral\n",
      "Sample 107: True = angry, Pred = angry\n",
      "Sample 108: True = sad, Pred = happy\n",
      "Sample 109: True = neutral, Pred = neutral\n",
      "Sample 110: True = disgust, Pred = happy\n",
      "Sample 111: True = neutral, Pred = sad\n",
      "Sample 112: True = fearful, Pred = angry\n",
      "Sample 113: True = sad, Pred = disgust\n",
      "Sample 114: True = disgust, Pred = disgust\n",
      "Sample 115: True = disgust, Pred = sad\n",
      "Sample 116: True = sad, Pred = fearful\n",
      "Sample 117: True = neutral, Pred = happy\n",
      "Sample 118: True = fearful, Pred = disgust\n",
      "Sample 119: True = fearful, Pred = angry\n",
      "Sample 120: True = sad, Pred = neutral\n",
      "Sample 121: True = neutral, Pred = neutral\n",
      "Sample 122: True = angry, Pred = neutral\n",
      "Sample 123: True = neutral, Pred = angry\n",
      "Sample 124: True = sad, Pred = angry\n",
      "Sample 125: True = angry, Pred = disgust\n",
      "Sample 126: True = happy, Pred = angry\n",
      "Sample 127: True = fearful, Pred = disgust\n",
      "Sample 128: True = angry, Pred = angry\n",
      "Sample 129: True = disgust, Pred = angry\n",
      "Sample 130: True = angry, Pred = disgust\n",
      "Sample 131: True = happy, Pred = happy\n",
      "Sample 132: True = angry, Pred = happy\n",
      "Sample 133: True = angry, Pred = fearful\n",
      "Sample 134: True = neutral, Pred = fearful\n",
      "Sample 135: True = sad, Pred = sad\n",
      "Sample 136: True = disgust, Pred = sad\n",
      "Sample 137: True = sad, Pred = happy\n",
      "Sample 138: True = fearful, Pred = angry\n",
      "Sample 139: True = neutral, Pred = angry\n",
      "Sample 140: True = happy, Pred = neutral\n",
      "Sample 141: True = disgust, Pred = angry\n",
      "Sample 142: True = fearful, Pred = happy\n",
      "Sample 143: True = fearful, Pred = happy\n",
      "Sample 144: True = happy, Pred = neutral\n",
      "Sample 145: True = fearful, Pred = angry\n",
      "Sample 146: True = sad, Pred = sad\n",
      "Sample 147: True = happy, Pred = neutral\n",
      "Sample 148: True = neutral, Pred = happy\n",
      "Sample 149: True = neutral, Pred = happy\n",
      "Sample 150: True = disgust, Pred = disgust\n",
      "Sample 151: True = happy, Pred = angry\n",
      "Sample 152: True = sad, Pred = fearful\n",
      "Sample 153: True = sad, Pred = happy\n",
      "Sample 154: True = angry, Pred = disgust\n",
      "Sample 155: True = happy, Pred = sad\n",
      "Sample 156: True = fearful, Pred = happy\n",
      "Sample 157: True = disgust, Pred = disgust\n",
      "Sample 158: True = happy, Pred = disgust\n",
      "Sample 159: True = neutral, Pred = neutral\n",
      "Sample 160: True = happy, Pred = angry\n",
      "Sample 161: True = happy, Pred = sad\n",
      "Sample 162: True = fearful, Pred = sad\n",
      "Sample 163: True = fearful, Pred = fearful\n",
      "Sample 164: True = fearful, Pred = happy\n",
      "Sample 165: True = happy, Pred = sad\n",
      "Sample 166: True = neutral, Pred = neutral\n",
      "Sample 167: True = disgust, Pred = disgust\n",
      "Sample 168: True = disgust, Pred = angry\n",
      "Sample 169: True = disgust, Pred = disgust\n",
      "Sample 170: True = disgust, Pred = angry\n",
      "Sample 171: True = neutral, Pred = happy\n",
      "Sample 172: True = sad, Pred = angry\n",
      "Sample 173: True = fearful, Pred = sad\n",
      "Sample 174: True = sad, Pred = happy\n",
      "Sample 175: True = angry, Pred = fearful\n",
      "Sample 176: True = disgust, Pred = neutral\n",
      "Sample 177: True = disgust, Pred = happy\n",
      "Sample 178: True = sad, Pred = disgust\n",
      "Sample 179: True = happy, Pred = fearful\n",
      "Sample 180: True = sad, Pred = angry\n",
      "Sample 181: True = neutral, Pred = sad\n",
      "Sample 182: True = angry, Pred = angry\n",
      "Sample 183: True = angry, Pred = sad\n",
      "Sample 184: True = angry, Pred = angry\n",
      "Sample 185: True = sad, Pred = happy\n",
      "Sample 186: True = disgust, Pred = sad\n",
      "Sample 187: True = disgust, Pred = fearful\n",
      "Sample 188: True = disgust, Pred = angry\n",
      "Sample 189: True = sad, Pred = happy\n",
      "Sample 190: True = angry, Pred = sad\n",
      "Sample 191: True = sad, Pred = neutral\n",
      "Sample 192: True = disgust, Pred = angry\n",
      "Sample 193: True = neutral, Pred = fearful\n",
      "Sample 194: True = neutral, Pred = angry\n",
      "Sample 195: True = neutral, Pred = neutral\n",
      "Sample 196: True = happy, Pred = disgust\n",
      "Sample 197: True = sad, Pred = disgust\n",
      "Sample 198: True = sad, Pred = angry\n",
      "Sample 199: True = disgust, Pred = angry\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Predictions vs True Labels ---\")\n",
    "label_map = {0: 'angry', 1: 'disgust', 2: 'fearful', 3: 'happy', 4: 'neutral', 5: 'sad'}\n",
    "\n",
    "for i, (t, p) in enumerate(zip(true_labels, pred_labels)):\n",
    "    print(f\"Sample {i:03}: True = {label_map[t.item()]}, Pred = {label_map[p.item()]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
